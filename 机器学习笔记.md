# 机器学习笔记

# 一、基础知识

## 1.凸优化

1. 目标函数的4个属性：1）是否为凸函数 2）是否连续 3）是否有限制条件 4）smooth（可微）

2. 凸优化问题的一般形式:

   $$\begin{align} min\ f_0(x) \\ s.t. \ f_i(x)>=0, \ i={1,2,...,m} \\ g_j(x)=0, \ j = {1,2,...,n} \end{align}$$

3. 凸函数性质

   1)   定义域是凸集 

   2）函数满足性质:   $\triangledown^2f(x)\geq0$ ，注意当x为向量时，对应的矩阵M是半正定矩阵，根据定义，对于任意向量$\bf v$，$\bf vMv^T>=0$，则M是半正定矩阵

## 2.矩阵论

1.协方差矩阵一定半正定



## 3.概率与统计

1.coxbox转换

$f(\lambda) = \begin{cases} \frac{x^\lambda-1}{\lambda},&\lambda \neq 0\\ln(x),&\lambda=0\\ \end{cases}$

$\lambda$的取值可以

2.各种数据标准化处理方式的对比

 <https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py>

https://sklearn.apachecn.org/docs/master/40.html?h=RobustScaler

|                     | 函数                      | 特点                                 |
| ------------------- | ------------------------- | ------------------------------------ |
| MinMaxScaler        | $y=(x-min)/(max-min)$     | 将数据限定在[0,1]                    |
| MaxAbsScaler        | $y = x / max(\mathbf x)$  | 适用于稀疏矩阵，因为不会破坏稀疏特性 |
| StandardScaler      | $y = (x-u)/\sigma $       | 标准方法                             |
| RobustScaler        | $y = (x-中位数)/四分位距$ | 适合于有异常值的数据                 |
| Normalizer          | 向量范数转化成1，一般用L2 |                                      |
| QuantileTransformer | 构建均匀分布              |                                      |
| PowerTransformer    | Coxbox                    | 让数据分布更加接近高斯分布           |



# 二、算法

## 2.1.LR线性回归

- LR为什么要用L2范数？而不是L1或者其他的？

  L1范数在存在某些不可微的点。L3、L5、...不是凸函数。

- LR为什么存在全局最优解？而不是局部最优解？

  L2是凸函数

- 正规方程求解?是否存在没有解的情况？怎么办？

  可能存在，SVD

  

- 用GD求最优解的时候会遇到什么挑战？SGD的优势是什么？实际是如何应用的？

  数据量太大时，优化需要用到所有的样本信息，导致速度变慢。

  SGD避免了上面的问题，大大提高优化速度，但是问题时无法收敛，最终可能在一定范围内波动。最终应用的时候需要随着误差的降低，减小学习率。

## 2.2.线性分类

1. 高斯判别模型

   [https://air-yan.github.io/machine%20learning/Generative-Learning-Algorithm/](https://air-yan.github.io/machine learning/Generative-Learning-Algorithm/)
   
2. 评估方法

   precision / recall / F1 / ROC AUC

   https://www.alexejgossmann.com/auc/ 

   auc的优势在于对不平衡的样本也具备很好的评估性能。auc的概率理解：从样本中任意选取一个正例和一个负例，正例的得分高于负例的得分的概率。所以它评估的是正负例的得分的排序，与阈值的选取没有关系。

3. 信息论里面的熵

   

## 2.3.SVM

- 凸优化的强对偶是怎么回事？弱对偶和强对偶的区别？

- 拉格朗日乘子法，为什么可以把带约束的优化问题转化成无约束的？

  对于问题 $argmin_x f(x),st.g(x)<=0,h(x)=0$，可以转化成

  $argmin_x\ argmax_{\lambda,u} \ L(x,\lambda,u) = f(x)+\lambda g(x)+u h(x),st.\lambda >=0$

  

- 满足kkt条件是强对偶的充分条件 

  kkt条件：

  1）$\frac{\partial L(x^*,\lambda^*,u^*)}{\partial x}=0$

  2）$\frac{\partial L(x^*,\lambda^*,u^*)}{\partial u}=0$

  3）$\lambda^*g(x^*) = 0$

  4）$\lambda^*>=0$

  5）$g(x^*)<=0$

  其中，$x^*,\lambda^*,u^*$ 是全局最优解

- 凸优化+slater/或者放松slater是强对偶的充分不必要条件

- 软间隔svm的正则项的系数怎么确定？取多少比较合适？

  <img src="/Users/fuhan/Library/Application Support/typora-user-images/image-20210225141651535.png" alt="image-20210225141651535" style="zoom:50%;" />

阿萨德

##2.4.核函数

成为kernel的条件，$G_{ij} = K(x_i,x_j)$ 满足如下性质：

1）G为对称矩阵 $G_{ij} = G{ji}$

2）G为半正定矩阵

常用核函数：多项式核、高斯核、...


## 2.5.PCA和LDA

- PCA用途：

  1）找出主要特征，剔除无关特征。简化以后的数据更容易分类 

  2）降低数据维度，简化计算、去噪

- PCA的推导

  关于协方差的含义：两个随机变量之间的关联程度。可以参考方差的定义，不同的是随机变量从1个变成2个。$cov(x_i,x_j)$

  对于协方差矩阵$C$，元素$C_{ij}=cov(x_i,x_j)$。假设对于一组随机变量$a_1,a_2,...,a_p$，有n个样本，构成样本空间$A\in R^{n*p}$，那么这p个随机变量的协方差矩阵可以写成$C = \frac{1}{n}B^TB$，其中$B$的定义见下面。

  PCA目的是为了让样本方差尽量大，或者重构代价最小。对于n个样本$x_1,x_2,...,x_n$，投影到某一个向量$u$，

  假设$u^Tu=1$，那么对于样本$x_i$，投影就是$x_i^Tu$。为了简化运算，我们首先对样本做一个去中心化的处理，$p = (x_1+x_2+...+x_n)/n$，样本变成$x_1-p,x_2-p,...,x_n-p$，投影$(x_i-p)^Tu$。把投影作为一个新的随机变量，计算均值u和方差D. 

  $U = \sum_{i=0}^n(u^T(x_i-p)) = u^T(\sum_{i=0}^n(x_i-p)) = u^T(x_1+x_2+...+x_n-n*p) = 0$ 

  $D = \frac{1}{n}\sum((x_i-p)^Tu-U)^2 = \frac{1}{n}\sum(u^T(x_i-p)(x_i-p)^Tu) \\=\frac{1}{n}u^TB^TBu= u^TSu $

  其中，$B = A-\mathbf1*p^T$，其中$\mathbf1=[1,1,...,1]_{1*n}^T$ ，$B$为$A$的去中心化矩阵

  这样原问题转变成:

  $f = argmax_{u} u^TSu \\ s.t:u^Tu=1$

  求解这个问题，用拉格朗日乘子法，加kkt条件。

  1）求$L(u,\lambda) = -u^TSu+\lambda(1-u^Tu)$

  2）$\frac{\partial L}{\partial u} = 0 $ $\frac{\partial L}{\partial \lambda} = 0 $

  解：$\lambda$ 和 u 分别为特征值和特征向量

- 核pca

  ![image-20210306200505052](/Users/fuhan/Library/Application Support/typora-user-images/image-20210306200505052.png)

- LDA用于将维度

  LDA降低维度的思想和PCA接近，同类别之间的数据方差尽量小，不同类别的数据的方差尽量大

  目标函数：

  $f = argmax_{u} \ \frac{u^TS_Bu}{u^TS_Wu} $ 

  转化成凸优化的标准形式：

  $f = argmin_{u} \ -u^TS_Bu \\ s.t. \ u^TS_Wu-1=0 $ 

  用拉格朗日函数 $L(u,\lambda)= -u^TS_Bu + \lambda(u^TS_Wu-1)$ 

  对u求偏微分，令其为0，解出来的u就是最优解

  $S_Bu = \lambda S_Wu$ , $S_W^{-1}S_Bu = \lambda u$ ，令$S_c*S_c = S_B$，由于$S_B$ 对称正定，必定可以对角化，且特征值为正实数，$S_c$存在。

  $S_cS_W^{-1}S_c*S_cu = \lambda S_cu$,$w=S_cu$,$S_cS_W^{-1}S_c*w = \lambda w$

## 2.6.推荐系统

1.推荐系统的类别？手动、简单的归类、个性化推荐

2.核心问题？

1）收集数据：

注意数据时效性

显性的数据的不完整，根据用户的行为来推断用户的反馈（隐形数据）

冷启动问题：对于新的物品和新的用户，没有数据

2）预测未知数据

3）如何评价推荐系统有多好？



## 2.7.集成模型

|      | Bagging             | Boosting            |
| ---- | ------------------- | ------------------- |
|      | Overfitting         | Underfitting        |
|      | lot of base learner | lot of base learner |
|      | 并行                | 串行                |

1.bagging  

随机森林

核心在于数据的随机性（比如又放回的boosttrap）、特性选取的随机性，用多个模型共同决策来提高预测的准确率，以及<u>降低预测结果的方差？</u>。



2.boosting

Adaboost

1）变更训练数据的权重 2）学习对应的模型的权重

![image-20210309120834700](/Users/fuhan/Library/Application Support/typora-user-images/image-20210309120834700.png)

GBDT：

1）串行学习 2）训练数据保持不变，变更标签。

3) 最终预测结果是所有模型输出的累加。

DT(决策树)：对于gdbt，使用的都是cart树



XGBT:[XGBT详解](https://jozeelin.github.io/2019/07/19/XGBoost/)

Light GBM：

|          | 特点                                               | 差异点                                                       |
| -------- | -------------------------------------------------- | ------------------------------------------------------------ |
| Adaboost | 根据模型误差自动调整实例以及模型权重的一种集成方法 |                                                              |
| GBDT     | 加法模型；利用目标函数的一阶导数近似               | 遍历所有的特征的所有可能切分点，耗时较多                     |
| XGBT     | 加法模型；利用目标函数的一阶和二阶导数近似         | 1.加入正则，防止过拟合 2.寻找切分点时，加入分位数，降低计算时间复杂度 3.列抽样，随机选取特征  4.特征预排序 |
| LightGBM |                                                    | 在gdbt的基础上做了几点优化：1.降低特征维度：对于直方图型的gdbt，将其中的0特征值做了处理 ；efb算法，对于一些互斥的特征(类似one-hot)进行合并 2.降低实例数量,goss算法。对于loss导数较小的实例，在选取时会忽略掉一部分。 |





##2.8.决策树

|      | 用途       | 特征选取                      |
| :--- | ---------- | ----------------------------- |
| ID3  | 分类问题   | 信息熵                        |
| C4.5 | 分类问题   | 在ID3的基础上增加了信息增益率 |
| CART | 分类、回归 | 分类用基尼系数、回归用MSE     |



## 2.9.聚类算法



|        | 目标函数                                             | 缺点                                                         | 优点                       |
| ------ | ---------------------------------------------------- | ------------------------------------------------------------ | -------------------------- |
| kmeans | 组内平方和最小                                       | 初始化不同，结果可能不同；kmeans++在初始化上进行了优化       |                            |
| EM     | E步，固定参数求下界函数；M步，最大化这个下界函数,... |                                                              |                            |
| DBSCAN |                                                      | 容易受到维度灾难的影响，在高纬度空间表现变差；无法区分不同密度的数据 | 不需要初始化；对噪音不敏感 |



## 2.10序列模型

跳转到nlp模型

## 2.11神经网络

### 2.11.1基本概念

输入：

隐含层：sigmoid\合页...

输出：输出层加一个sigmoid或者是softmax解决分类问题

反向传播：原理是对损失函数求导，会用到一些矩阵求导的知识

卷积：本质上简化了在输入和隐含层之间的线性处理部分（全连接层），对于图像而言，相邻的像素之间是存在强关联的，在计算隐含层的输入时，不需要用到所有的特征，计算的复杂度大大降低。

padding：引入这个的原因1）图像边缘的像素参数计算的次数只有一次，可能损失部分信息 2）避免图像经过卷积之后变小太多。

池化：提高鲁棒性

### 2.11.2算法分类

> 1.目标检测：分类算法，判断图中是否存在某种类型的物体
>
> 卷积神经网络类似于滑动窗口的办法，检测每一个窗口是否存在目标
>
> 2.目标定位：除了分类，还增加了物体的边界信息

yolo算法：目标定位算法

### 2.11.3 人脸识别

人脸识别是1:k的问题，关键问题在于解决单个人的样本很少的问题。

如何设定损失函数？

###2.11.4风格迁移

https://arxiv.org/abs/1508.06576 论文链接

### 2.11.5 卷积的推广

###2.11.6 正则化

Dropout:训练阶段对网络某一层的所有的神经元都以某个概率p丢弃。由于会影响下一层的网络的输入，在预测阶段需要停掉dropout,但是为了保持和训练阶段一致，需要对该层的神经元输出乘上系数p,

batch norm : 对batch中的所有的实例，保持所有的特征的均值和方差一致，尽量让分布保持一致。 

L1/L2正则

##2.12 NLP领域相关的算法

### 2.12.1 HMM隐式马尔可夫

1）有向图模型

2）概念：发射概率$B$、转换概率$A$、初始概率$\pi$ ，$\theta = (A,B,\pi)$；$z$ 隐藏状态，$x$ 观测状态

3） 三类常见的任务

> 1）已知状态$x$和参数$\theta$ ,求$z$ ，viterbi（本质dp算法）
>
> 2）已知状态$x$，推测参数$\theta$ ，EM算法
>
> 3）已知$\theta$,推测$x$，一般dp算法 

4）EM算法如何计算

>分成两步：
>
>1）E步，假设已知$\theta$，求z
>
>$p(z_k|X) = p(z_k,X)/p(X)$
>
>$ p(z_k,X) = p(z_k,x_{1:k},x_{k+1:n}) = p(z_k,x_{1:k})*p(x_{k+1:n}|z_k)$ ##这里用到了图的一些知识
>
>上式$p(z_k,x_{1:k})$ 和p(x_{k+1:n}|z_k)部分的计算都可以使用dp算法。
>
>然后计算$p(z_k,z_{k+1}|X)$,计算过程类似$p(z_k|X)$，先计算$p(z_k,z_{k+1},X) = p(z_k,x_{1:k})*p(x_{k+2:n}|z_{k+1})*p(z_{k+1}|z_k)*p(x_{k+1}|z_{k+1})$,这4个部分前面已经计算得到，最后经过归一化处理，即可得到$p(z_k,z_{k+1}|X)$
>
>2）M步，最大似然估计
>
>遍历所有的样本，统计不同的转移出现的次数。用最大似然估计的思想得到转移矩阵$A$

### 2.12.2 CRF条件随机场

|      |                    |                    |
| ---- | ------------------ | ------------------ |
| CRF  | 无向图、全局归一化 | 引入团和分数的概念 |
| HMM  | 有向图、局部归一化 | 计算用的是联合概率 |

HMM存在的问题：现实中不同时刻的观测状态可能不仅仅由当前的隐藏状态决定，还可能跟其他时刻相关。

MEMM：局部归一化可能导致label bias问题，为了解决这个问题，将有向图改成无向图

![image-20210606124820565](/Users/fuhan/Library/Application Support/typora-user-images/image-20210606124820565.png)

![image-20210606125135001](/Users/fuhan/Library/Application Support/typora-user-images/image-20210606125135001.png)



CRF的基本流程：

1）把图分成n个团，对于每个团有一个score函数$\phi(x)$，联合概率$p=\frac{1}{Z}*\Pi_i^n\phi_i$,Z是归一化处理

2）构建特征

3）利用这些特征定义函数$\phi(x)$

![image-20210606114448466](/Users/fuhan/Library/Application Support/typora-user-images/image-20210606114448466.png)

下面的推导可以看出多分类的LR可以从无向图模型推导出来。

![image-20210606115920912](/Users/fuhan/Library/Application Support/typora-user-images/image-20210606115920912.png)

LR模型就是log-linear模型，相比于CRF的不同之处在于score函数如何定义。



### 2.12.3词向量

基本思想：基于单词的上下文来描述这个单词

相比于one-hot来描述某个单词，基于向量的表示方法，有利于后续比较两个词语的相似度。

基本实现：

1）为每一个单词初始化两个维度一样的向量，u和v ，u表示作为临近词汇，v表示作为中心词汇。

2）$p(w_{u_j}|w_{v_i})=exp(w^T_{u_j}w_{v_i})/norm$ 表示 中心词汇$v_i$ 时，其临近词汇$u_j$出现的概率。norm是归一化处理，对$u_j$在词汇表中遍历所有可能性。（无向图模型的score方法）

3）计算中心词汇是$v_i$时，临近的几个词语$u_j，j=[i-k,...i-1,i+1,...i+k]$出现的概率。

4）对整个文本使用方法3)，并将所有的概率相乘

5）使用反向梯度对损失函数的参数进行优化，参数就是u和v

6）最后得到的向量u和v平均就是单个词汇的词向量



skip-gram  & cbow是两个变种，根据cs224的课程课件，选择skip-gram

https://zhuanlan.zhihu.com/p/27234078 

1）negative-sample

可以看到归一化的计算涉及到整个词库所有的词汇，极大增大计算复杂度，改进模型如下：

<img src="/Users/fuhan/Library/Application Support/typora-user-images/image-20210605205553855.png" style="zoom:30%;" />

$j$满足的分布P(w)：

<img src="/Users/fuhan/Library/Application Support/typora-user-images/image-20210605205655091.png" alt="image-20210605205655091" style="zoom:40%;" />

也就是说，对于没有出现在窗口中的词汇，先计算其出现的频次，然后根据频次计算其出现的概率，最后根据概率采样k次。



### 2.12.2 RNN

对于语言模型：预测接下来的单词 

一种基本的算法n-gram，传统的统计机器学习，统计样本中短语出现的次数，把次数最大的单词作为预测值

|      | N-gram                   | RNN                      |
| ---- | ------------------------ | ------------------------ |
|      | 固定窗口                 | 对输入尺寸没有限制       |
|      | 可能存在样本不充分的问题 | 隐含层提炼出更高级的特征 |
|      |                          | 可以用到之前的词语的信息 |

![image-20210606163337387](/Users/fuhan/Library/Application Support/typora-user-images/image-20210606163337387.png)

> 训练rnn的过程
>
> 1）输入数据：($x_1,x_2,...,x_t$)作为input，$x_{t+1}$作为标签，交叉熵作为损失函数在t时刻的损失，累加t=1,2,...，即可得到总的损失函数。
>
> 2）利用梯度下降优化网络参数。为了提高训练速度，可以把整个文本，按照段落或者句子分成batch，采用sgd优化。

如何验证算法的有效性？可以看出下面的这个指标等效于$exp(J)$

![image-20210606165510968](/Users/fuhan/Library/Application Support/typora-user-images/image-20210606165510968.png)





###2.12.3 LSTM/GRU

rnn梯度消失问题的原因：共用一个W参数矩阵，当输入的样本长度足够大时，深度会足够大，如果这个w很小，会导致梯度随着深度的增大而下降

rnn梯度爆炸：用剪枝的方法限制

为了缓解rnn的梯度消失的问题，以及两个节点之间的信息传递会随着节点之间的距离的增大而降低的问题，引入了记忆机制。

![image-20210606180447449](/Users/fuhan/Library/Application Support/typora-user-images/image-20210606180447449.png)

![image-20210606181837260](/Users/fuhan/Library/Application Support/typora-user-images/image-20210606181837260.png)

### 2.12.4 双向模型/多层模型

从RNN的模型来看，存在单向的依赖，$p(x_t)$只跟t之前的时间节点相关。实际上，在预测下一个单词时，也可能跟t之后的节点相关。所以引入了双向的概念

多层RNN：

###2.12.5 常见的nlp任务

1）序列标注

比如NER

Baseline（传统机器学习）：HMM、CRF

Baseline（深度学习）：CNN+BiLSYTM+CRF

论文地址https://www.aclweb.org/anthology/Q16-1026.pdf

代码https://github.com/kamalkraj/Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs

2）sentence classify

3）QA/机器翻译

## 2.13 推荐系统

1. 基于内容
2. 协同过滤
3. 矩阵分解
4. 因子分解机（Factorization Machine)
5. 深度学习
6. 机器学习排序(Learning to Rank)
7. 探索与利用(Explore and exploit): Contextual bandit
8. 集成学习 (Ensemble/Hybrid method)



![image-20210607154034152](/Users/fuhan/Library/Application Support/typora-user-images/image-20210607154034152.png)



# 三、特征工程



# 四、线上部署

## 3.1 常见的生产环境

https://baijiahao.baidu.com/s?id=1668892506436369087&wfr=spider&for=pc

## 3.2常用的模型上线的方法

基于api预测的案例(flask)https://github.com/AndrewFull/flask_api，这个方法属于基础方法，测试的时候可以使用。

PMML：https://zhuanlan.zhihu.com/p/79197337 里面有详细的案例

|                    | 优缺点                                                       | 常用的包                                  |
| ------------------ | ------------------------------------------------------------ | ----------------------------------------- |
| pkl                | 安全性和可维护性的局限性                                     | joblib、pickle:一般而言joblib效率相对更高 |
| Pmml               | 跨平台开发，**在模型上线的过程中，PMML经常作为中间媒介连接离线训练平台和线上预测平台** | 参考上面的链接                            |
| TensorFlow Serving | 原理和pmml类似                                               |                                           |





# 附录

##1.linux实用技巧

### 1.1常用指令

https://zhuanlan.zhihu.com/p/80578535

### 1.2如何在云服务器终端上运行jupyter notebook

1)本地运行

```
ssh -L 8000:localhost:8888 <<user name>>@<<ip of remote machine>>
```

2)终端

```
jupyter notebook --ip=127.0.0.1 --port=8888 --no-browser
```

运行之后会得到类似下面的信息，将8888改成8000即可

```
http://127.0.0.1:8888/?token=<<token, i,.e, bunch of numbers>>
```



## 2.SPARK

### 2.1基本概念

Spark:快速、集群计算

http://dblab.xmu.edu.cn/blog/1709-2/

这个很基础，适合入门

### 2.2 hadoop环境搭建

http://dblab.xmu.edu.cn/blog/install-hadoop/ 

启动命令可以用下面两个

```
./sbin/hadoop-daemon.sh start namenode

./sbin/hadoop-daemon.sh start datanode
```

通过网址 [http://127.0.0.1:50070](http://127.0.0.1:50070/dfshealth.html#tab-overview)

###2.3 spark集群的搭建

1)下载和解压安装

```
wget (spark官网地址) #下载安装包
tar -xzvf (spark安装包) -C (安装地址)
```

2）配置spark-env.sh

```
cd （spark的安装目录）# 找到文件所在的目录
vim conf/spark-env.sh 
```

在其中添加如下的内容 

```
export JAVA_HOME=/...  # 添加java的安装目录 
export SPARK_MASTER_HOST=(主机的hostname,可以通过在终端输入hostname查看)
export SPARK_MASTER_PORT=7077（这个值貌似看自己需求？）
```

3)配置workers文件

```
cd （spark的安装目录）/conf
vim workers #如果没有创建一个
```

添加如下的代码

```
#删除localhost这一行
(主节点hostname) #这个可以不需要
(从节点1hostname)
(从节点2hostname)
```

4）historyserver的配置

https://www.bilibili.com/video/BV1ui4y1V7Cf?p=8参考链接

5) 分发和启动

```
scp  -r  (spark的安装目录)  (nodename#其他的节点的hostname):$PWD
```

```
cd (spark安装目录)/sbin
start-all.sh
jps #	查看
```

6）利用zookeeper的standalone的高可用配置

### 2.4 RDD

rdd的容错机制？1）缓存/checkpoint 2）记录依赖关系和计算方式

rdd的算子分类：1）transform ，生成图  2)action，触发图的计算

rdd为什么只读呢？满足设计需求：惰性求值、容错等等



## 3.pytorch

###3.1 tensorboard

如何利用vs code连接阿里云服务器，在服务器上运行机器学习，并在本地浏览器上打开tensorboard

1）本地安装vscode，远程连接服务器公网ip地址。连接上以后，可以在本地访问服务器文件，打开jupyter notebook（需要在远程安装python的扩展包）

![image-20210604092719389](/Users/fuhan/Library/Application Support/typora-user-images/image-20210604092719389.png)

2）在打开的jupyter 中运行如下命令，提示可以通过6006端口可以访问

```jupyter notebook/python
!tensorboard --logdir=runs
```

![image-20210604091337766](/Users/fuhan/Library/Application Support/typora-user-images/image-20210604091337766.png)

3）在本地终端输入，将本地的8000端口连接到服务器的6006端口，会提示输入密码，连接即可

```
ssh -L 8000:localhost:6006 hadoop@47.96.172.112
```

4）在本地浏览器打开http://127.0.0.1:8000/即可

###3.2 PytorchScript


# 机器学习笔记

# 一、基础知识

## 1.凸优化

1. 目标函数的4个属性：1）是否为凸函数 2）是否连续 3）是否有限制条件 4）smooth（可微）

2. 凸优化问题的一般形式:

   $$\begin{align} min\ f_0(x) \\ s.t. \ f_i(x)>=0, \ i={1,2,...,m} \\ g_j(x)=0, \ j = {1,2,...,n} \end{align}$$

3. 凸函数性质

   1)   定义域是凸集 

   2）函数满足性质:   $\triangledown^2f(x)\geq0$ ，注意当x为向量时，对应的矩阵M是半正定矩阵，根据定义，对于任意向量$\bf v$，$\bf vMv^T>=0$，则M是半正定矩阵

## 2.矩阵论

1.协方差矩阵一定半正定

# 二、算法

## 1.LR线性回归

- LR为什么要用L2范数？而不是L1或者其他的？

  L1范数在存在某些不可微的点。L3、L5、...不是凸函数。

- LR为什么存在全局最优解？而不是局部最优解？

  L2是凸函数

- 正规方程求解?是否存在没有解的情况？怎么办？

  可能存在，SVD

  

- 用GD求最优解的时候会遇到什么挑战？SGD的优势是什么？实际是如何应用的？

  数据量太大时，优化需要用到所有的样本信息，导致速度变慢。

  SGD避免了上面的问题，大大提高优化速度，但是问题时无法收敛，最终可能在一定范围内波动。最终应用的时候需要随着误差的降低，减小学习率。

## 2.线性分类



1. 高斯判别模型

   <[https://air-yan.github.io/machine%20learning/Generative-Learning-Algorithm/](https://air-yan.github.io/machine learning/Generative-Learning-Algorithm/)>

## 3.SVM

- 凸优化的强对偶是怎么回事？弱对偶和强对偶的区别？

- 拉格朗日乘子法，为什么可以把带约束的优化问题转化成无约束的？

  对于问题 $argmin_x f(x),st.g(x)<=0,h(x)=0$，可以转化成

  $argmin_x\ argmax_{\lambda,u} \ L(x,\lambda,u) = f(x)+\lambda g(x)+u h(x),st.\lambda >=0$

  

- 满足kkt条件是强对偶的充分条件 

  kkt条件：

  1）$\frac{\partial L(x^*,\lambda^*,u^*)}{\partial x}=0$

  2）$\frac{\partial L(x^*,\lambda^*,u^*)}{\partial u}=0$

  3）$\lambda^*g(x^*) = 0$

  4）$\lambda^*>=0$

  5）$g(x^*)<=0$

  其中，$x^*,\lambda^*,u^*$ 是全局最优解

- 凸优化+slater/或者放松slater是强对偶的充分不必要条件

- 软间隔svm的正则项的系数怎么确定？取多少比较合适？

  <img src="/Users/fuhan/Library/Application Support/typora-user-images/image-20210225141651535.png" alt="image-20210225141651535" style="zoom:50%;" />

阿萨德

##4.核函数

成为kernel的条件，$G_{ij} = K(x_i,x_j)$ 满足如下性质：

1）G为对称矩阵 $G_{ij} = G{ji}$

2）G为半正定矩阵

常用核函数：多项式核、高斯核、...


## 5.PCA和LDA

- PCA用途：

  1）找出主要特征，剔除无关特征。简化以后的数据更容易分类 

  2）降低数据维度，简化计算、去噪

- PCA的推导

  关于协方差的含义：两个随机变量之间的关联程度。可以参考方差的定义，不同的是随机变量从1个变成2个。$cov(x_i,x_j)$

  对于协方差矩阵$C$，元素$C_{ij}=cov(x_i,x_j)$。假设对于一组随机变量$a_1,a_2,...,a_p$，有n个样本，构成样本空间$A\in R^{n*p}$，那么这p个随机变量的协方差矩阵可以写成$C = \frac{1}{n}B^TB$，其中$B$的定义见下面。

  PCA目的是为了让样本方差尽量大，或者重构代价最小。对于n个样本$x_1,x_2,...,x_n$，投影到某一个向量$u$，

  假设$u^Tu=1$，那么对于样本$x_i$，投影就是$x_i^Tu$。为了简化运算，我们首先对样本做一个去中心化的处理，$p = (x_1+x_2+...+x_n)/n$，样本变成$x_1-p,x_2-p,...,x_n-p$，投影$(x_i-p)^Tu$。把投影作为一个新的随机变量，计算均值u和方差D. 

  $U = \sum_{i=0}^n(u^T(x_i-p)) = u^T(\sum_{i=0}^n(x_i-p)) = u^T(x_1+x_2+...+x_n-n*p) = 0$ 

  $D = \frac{1}{n}\sum((x_i-p)^Tu-U)^2 = \frac{1}{n}\sum(u^T(x_i-p)(x_i-p)^Tu) \\=\frac{1}{n}u^TB^TBu= u^TSu $

  其中，$B = A-\mathbf1*p^T$，其中$\mathbf1=[1,1,...,1]_{1*n}^T$ ，$B$为$A$的去中心化矩阵

  这样原问题转变成:

  $f = argmax_{u} u^TSu \\ s.t:u^Tu=1$

  求解这个问题，用拉格朗日乘子法，加kkt条件。

  1）求$L(u,\lambda) = -u^TSu+\lambda(1-u^Tu)$

  2）$\frac{\partial L}{\partial u} = 0 $ $\frac{\partial L}{\partial \lambda} = 0 $

  解：$\lambda$ 和 u 分别为特征值和特征向量

- 核pca

  ![image-20210306200505052](/Users/fuhan/Library/Application Support/typora-user-images/image-20210306200505052.png)

- LDA用于将维度

  LDA降低维度的思想和PCA接近，同类别之间的数据方差尽量小，不同类别的数据的方差尽量大

  目标函数：

  $f = argmax_{u} \ \frac{u^TS_Bu}{u^TS_Wu} $ 

  转化成凸优化的标准形式：

  $f = argmin_{u} \ -u^TS_Bu \\ s.t. \ u^TS_Wu-1=0 $ 

  用拉格朗日函数 $L(u,\lambda)= -u^TS_Bu + \lambda(u^TS_Wu-1)$ 

  对u求偏微分，令其为0，解出来的u就是最优解

  $S_Bu = \lambda S_Wu$ , $S_W^{-1}S_Bu = \lambda u$ ，令$S_c*S_c = S_B$，由于$S_B$ 对称正定，必定可以对角化，且特征值为正实数，$S_c$存在。

  $S_cS_W^{-1}S_c*S_cu = \lambda S_cu$,$w=S_cu$,$S_cS_W^{-1}S_c*w = \lambda w$

## 6.推荐系统

1.推荐系统的类别？手动、简单的归类、个性化推荐

2.核心问题？

1）收集数据：

注意数据时效性

显性的数据的不完整，根据用户的行为来推断用户的反馈（隐形数据）

冷启动问题：对于新的物品和新的用户，没有数据

2）预测未知数据

3）如何评价推荐系统有多好？



## 7.集成模型

|      | Bagging             | Boosting            |
| ---- | ------------------- | ------------------- |
|      | Overfitting         | Underfitting        |
|      | lot of base learner | lot of base learner |
|      | 并行                | 串行                |

1.bagging  

随机森林

核心在于数据的随机性（比如又放回的boosttrap）、特性选取的随机性，用多个模型共同决策来提高预测的准确率，以及<u>降低预测结果的方差？</u>。



2.boosting

Adaboost

1）变更训练数据的权重 2）学习对应的模型的权重

![image-20210309120834700](/Users/fuhan/Library/Application Support/typora-user-images/image-20210309120834700.png)

GBDT：

1）串行学习 2）训练数据保持不变，变更标签。

3) 最终预测结果是所有模型输出的累加。

DT(决策树)：对于gdbt，使用的都是cart树



XGBT:[XGBT详解](https://jozeelin.github.io/2019/07/19/XGBoost/)

Light GBM：

|          | 特点                                               | 差异点                                                       |
| -------- | -------------------------------------------------- | ------------------------------------------------------------ |
| Adaboost | 根据模型误差自动调整实例以及模型权重的一种集成方法 |                                                              |
| GBDT     | 加法模型；利用目标函数的一阶导数近似               | 遍历所有的特征的所有可能切分点，耗时较多                     |
| XGBT     | 加法模型；利用目标函数的一阶和二阶导数近似         | 1.加入正则，防止过拟合 2.寻找切分点时，加入分位数，降低计算时间复杂度 3.列抽样，随机选取特征  4.特征预排序 |
| LightGBM |                                                    | 在gdbt的基础上做了几点优化：1.降低特征维度：对于直方图型的gdbt，将其中的0特征值做了处理 ；efb算法，对于一些互斥的特征(类似one-hot)进行合并 2.降低实例数量,goss算法。对于loss导数较小的实例，在选取时会忽略掉一部分。 |



##8.决策树

|      | 用途       | 特征选取                      |
| :--- | ---------- | ----------------------------- |
| ID3  | 分类问题   | 信息熵                        |
| C4.5 | 分类问题   | 在ID3的基础上增加了信息增益率 |
| CART | 分类、回归 | 分类用基尼系数、回归用MSE     |



## 9.聚类算法



|        | 目标函数                                             | 缺点                                                         | 优点                       |
| ------ | ---------------------------------------------------- | ------------------------------------------------------------ | -------------------------- |
| kmeans | 组内平方和最小                                       | 初始化不同，结果可能不同；kmeans++在初始化上进行了优化       |                            |
| EM     | E步，固定参数求下界函数；M步，最大化这个下界函数,... |                                                              |                            |
| DBSCAN |                                                      | 容易受到维度灾难的影响，在高纬度空间表现变差；无法区分不同密度的数据 | 不需要初始化；对噪音不敏感 |



## 10.特征工程

